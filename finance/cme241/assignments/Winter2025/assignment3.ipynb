{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2025) - Assignment 3\n",
    "\n",
    "**Due: Sunday, February 23 @ 11:59 PM PST on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- Make sure each of the subquestions have answers\n",
    "- Ensure that group members indicate which problems they're in charge of\n",
    "- Show work and walk through your thought process where applicable\n",
    "- Empty code blocks are for your use, so feel free to create more under each section as needed\n",
    "- Document code with light comments (i.e. 'this function handles visualization')\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/my-username/my-repo/assignment-file-name.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "- Person 1\n",
    "- Person 2\n",
    "- Person 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize_scalar\n",
    "from typing import Iterable, Iterator, Mapping, TypeVar, Callable, Sequence, Tuple, Dict\n",
    "from rl.distribution import Categorical\n",
    "from rl.markov_process import FiniteMarkovRewardProcess\n",
    "from rl.chapter10.prediction_utils import compare_td_and_mc\n",
    "X = TypeVar('X')\n",
    "S = TypeVar('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Milk Vendor Optimization Problem (Led by ______)\n",
    "\n",
    "You are a milk vendor and your task is to bring to your store a supply (denoted $S \\in \\mathbb{R}$) of milk volume in the morning that will give you the best profits. You know that the demand for milk throughout the course of the day is a probability distribution function $f$ (for mathematical convenience, assume people can buy milk in volumes that are real numbers, hence milk demand $x \\in \\mathbb{R}$ is a continuous variable with a probability density function). \n",
    "\n",
    "For every extra gallon of milk you carry at the end of the day (supply $S$ exceeds random demand $x$), you incur a cost of $h$ (effectively the wasteful purchases amounting to the difference between your purchase price and the end-of-day discount disposal price since you are not allowed to sell the same milk the next day). For every gallon of milk that a customer demands that you don’t carry (random demand $x$ exceeds supply $S$), you incur a cost of $p$ (effectively the missed sales revenue amounting to the difference between your sales price and purchase price). \n",
    "\n",
    "Your task is to identify the optimal supply $S$ that minimizes your **Expected Cost** $g(S)$, given by:\n",
    "\n",
    "$$\n",
    "g_1(S) = \\mathbb{E}[\\max(x - S, 0)] = \\int_{S}^{\\infty} (x - S) \\cdot f(x) \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_2(S) = \\mathbb{E}[\\max(S - x, 0)] = \\int_{-\\infty}^{S} (S - x) \\cdot f(x) \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "g(S) = p \\cdot g_1(S) + h \\cdot g_2(S)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): Analytical Optimization\n",
    "\n",
    "1. **Derive the first-order condition (FOC)** for minimizing the expected cost $g(S)$.\n",
    "2. **Solve the FOC** to express the optimal supply $S^*$ in terms of the given parameters: $p$, $h$, and the demand distribution $f(x)$. (*Hint*: Pay attention to the balance between the costs of overstocking and understocking)\n",
    "\n",
    "3. **Interpretation**: Provide an interpretation of the condition you derived. What does the balance between $p$ and $h$ imply about the optimal supply $S^*$?\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Special Case Analysis\n",
    "\n",
    "1. Consider the case where the demand $x$ follows an **exponential distribution** with parameter $\\lambda > 0$. That is, $f(x) = \\lambda e^{-\\lambda x}$ for $x \\geq 0$.\n",
    "    - Derive an explicit expression for the optimal supply $S^*$.\n",
    "    \n",
    "2. Consider the case where the demand $x$ follows a **normal distribution** with mean $\\mu$ and variance $\\sigma^2$, i.e., $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$. \n",
    "    - Set up the integral for $g(S)$ and describe how it relates to the **cumulative distribution function (CDF)** of the normal distribution.\n",
    "    - Provide an interpretation of how changes in $\\mu$ and $\\sigma$ influence the optimal $S^*$. \n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Framing as a Financial Options Problem\n",
    "\n",
    "1. Frame the milk vendor’s problem as a **portfolio of call and put options**:\n",
    "    - Identify the analog of the “strike price” and “underlying asset.”\n",
    "    - Explain which part of the cost function $g_1(S)$ or $g_2(S)$ corresponds to a call option and which part to a put option.\n",
    "    - What do $p$ and $h$ represent in this options framework?\n",
    "\n",
    "2. Explain how this framing could be used to derive the optimal supply $S^*$ if solved using financial engineering concepts.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (D): Numerical Solution and Simulation\n",
    "\n",
    "1. **Numerical Solution**: Write a Python function that numerically estimates the optimal $S^*$ using an iterative search or numerical optimization method. \n",
    "\n",
    "2. **Simulation**: Generate random samples of milk demand from an exponential distribution and simulate the total costs for different values of $S$. Plot the costs against $S$ and visually identify the optimal $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fill in the code below, and then run the code in the next cell for the simulation\n",
    "'''\n",
    "\n",
    "# simulation parameters\n",
    "p = 5  # Cost of understocking (per unit)\n",
    "h = 2  # Cost of overstocking (per unit)\n",
    "lambda_param = 1.0  # Parameter for exponential distribution\n",
    "\n",
    "# Probability density function for demand\n",
    "def demand_pdf(x):\n",
    "    return _\n",
    "\n",
    "# Cumulative distribution function for demand\n",
    "def demand_cdf(x):\n",
    "    return _\n",
    "\n",
    "# Expected cost function g(S)\n",
    "def expected_cost(S):\n",
    "    # g1(S): Understocking cost\n",
    "    g1 = _\n",
    "    \n",
    "    # g2(S): Overstocking cost (integral using CDF)\n",
    "    g2 = _\n",
    "    \n",
    "    return g1 + g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = minimize_scalar(expected_cost, bounds=(0, 10), method='bounded')\n",
    "optimal_S = result.x\n",
    "\n",
    "# Simulation of costs\n",
    "S_values = np.linspace(0, 10, 500)\n",
    "costs = [expected_cost(S) for S in S_values]\n",
    "\n",
    "# Plotting the costs against S\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(S_values, costs, label=\"Expected Cost $g(S)$\", color='blue')\n",
    "plt.axvline(optimal_S, color='red', linestyle='--', label=f\"Optimal $S^* \\\\approx {optimal_S:.2f}$\")\n",
    "plt.xlabel(\"Supply $S$\")\n",
    "plt.ylabel(\"Expected Cost $g(S)$\")\n",
    "plt.title(\"Expected Cost $g(S)$ vs. Supply $S$ (Exponential Demand)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "optimal_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Optimal Bank Cash Management with Risky Investments and Penalties (Led By: _____)\n",
    "\n",
    "Assume you are the owner of a bank where customers come in randomly every day to make cash deposits and to withdraw cash from their accounts. At the end of each day, you can borrow (from another bank, without transaction costs) any cash amount $y > 0$ at a constant daily interest rate $R$, meaning you will need to pay back a cash amount of $y(1 + R)$ at the end of the next day. Also, at the end of each day, you can invest a portion of your bank’s cash in a risky (high return, high risk) asset. Assume you can change the amount of your investment in the risky asset each day, with no transaction costs (this is your mechanism to turn any amount of cash into risky investment or vice-versa).\n",
    "\n",
    "A key point here is that once you make a decision to invest a portion of your cash in the risky asset at the end of a day, you will not have access to this invested amount as cash that otherwise could have been made available to customers who come in the next day for withdrawals. More importantly, if the cash amount $c$ in your bank at the start of a day is less than $C$, the banking regulator will make you pay a penalty of $K \\cdot \\cot\\left( \\frac{\\pi c}{2C} \\right)$ (for a given constant $K > 0$).\n",
    "\n",
    "For convenience, we make the following assumptions:\n",
    "- Assume that the borrowing and investing is constrained so that we end the day (after borrowing and investing) with positive cash ($c > 0$) and that any amount of regulator penalty can be immediately paid (meaning $c \\geq K \\cdot \\cot\\left( \\frac{\\pi c}{2C} \\right)$ when $c \\leq C$).\n",
    "- Assume that the deposit rate customers earn is so small that it can be ignored.\n",
    "- Assume for convenience that the first half of the day is reserved for only depositing money and the second half of the day is reserved for only withdrawal requests.\n",
    "- Assume that if you do not have sufficient cash to fulfill a customer withdrawal request, you ask the customer to make the withdrawal request again the next day.\n",
    "- Assume all quantities are continuous variables.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): MDP Modeling\n",
    "\n",
    "Model the bank’s problem as a **Markov Decision Process (MDP)** with the goal of maximizing the **Expected Utility of assets less liabilities** at the end of a $T$-day horizon, conditional on any current situation of assets and liabilities.\n",
    "\n",
    "1. **State Space**: Define the possible states of the system.\n",
    "2. **Action Space**: Specify the possible actions available to the bank at each state.\n",
    "3. **Transition Function**: Describe how the state evolves based on the current state and the chosen action.\n",
    "4. **Reward Function**: Specify the reward structure that incentivizes optimal behavior.\n",
    "\n",
    "*Note*: Be very careful with your notation; ensure that every subscript, index, superscript, prime, etc. is properly defined and necessary. There are a lot of variables at play, so everything must be properly defined or points will be deducted.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Reinforcement Learning Approach\n",
    "\n",
    "In a practical setting, we do not know the exact probability distributions of the customer deposits and withdrawals. Neither do we know the exact stochastic process of the risky asset. But assume we have access to a large set of historical data detailing daily customer deposits and withdrawal requests, as well as daily historical market valuations of the risky asset. Assume we also have data on new customers as well as leaving customers (sometimes due to their withdrawal requests not being satisfied promptly).\n",
    "\n",
    "Describe your approach to solving this problem with **Reinforcement Learning** by using the historical data described above.\n",
    "\n",
    "1. Specify which **Reinforcement Learning algorithm** you would use, including any customizations for this problem.\n",
    "2. Provide sufficient detail that will enable a programmer with knowledge of RL to implement your ideas.\n",
    "\n",
    "*Note*: You are not expected to write any code for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Career Optimization (Led By: _____)\n",
    "\n",
    "Imagine you live in a world where every job is an hourly-wage job. You have $H$ available hours in a day (for some fixed $H \\in \\mathbb{Z}^+$), and each morning when you leave your house, you can decide to split those $H$ hours into:\n",
    "\n",
    "- Hours spent on learning to get better at your current job (call it $l \\in \\mathbb{Z}_{\\geq 0}$),\n",
    "- Hours spent on searching for another job (call it $s \\in \\mathbb{Z}_{\\geq 0}$), and\n",
    "- Remaining $H - l - s$ hours spent on actually working on your current job.\n",
    "\n",
    "If your job currently pays you at an hourly-wage of $w$ dollars, then at the end of that day, you will be given a cash amount of $w \\cdot (H - l - s)$ dollars. We assume that any hourly-wage $w$ in our world is an integer in the finite set $\\{1, 2, \\dots, W\\}$ for some fixed $W \\in \\mathbb{Z}^+$.\n",
    "\n",
    "Each employer has a wage model such that if you spend $l$ hours on learning on a given day where your hourly-wage was $w$, then the employer sends you an email the next morning with that new day’s hourly-wage of:  \n",
    "$$\\min(w + x, W)$$  \n",
    "where $x$ is a Poisson random variable with mean $\\alpha \\cdot l$ for some fixed $\\alpha \\in \\mathbb{R}^+$.\n",
    "\n",
    "Each morning, with probability $\\frac{\\beta s}{H}$ for some fixed $\\beta \\in [0, 1]$, you will receive an email from another employer with a job-offer with hourly-wage of  \n",
    "$$\\min(w + 1, W)$$  \n",
    "where $w$ was the hourly wage of the job you were on the previous day and $s$ is the number of hours you spent on job-search the previous day.\n",
    "\n",
    "You read all your emails before you leave your house in the morning. If another job is offered to you and if the hourly-wage of that job is greater than your current employer’s hourly-wage stated in that morning’s email, then you accept the other job. Otherwise, you continue in your current job. Whichever job you decide to do, each morning when you leave your house, you decide how to split the $H$ hours of that day into learning hours, job-searching hours, and working hours.\n",
    "\n",
    "Your goal is to maximize the **Expected (Discounted) Wages** earned over an infinite horizon (assume you never age and will live infinitely). The daily discount factor is a fixed $0 < \\gamma < 1$.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): MDP Modeling\n",
    "\n",
    "With proper mathematical notation, model this as a **Finite MDP** specifying the states, actions, rewards, state-transition probabilities, and discount factor. Be very precise with your notation!!\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Python Implementation\n",
    "\n",
    "Implement this MDP in Python. If you wish, you may use the code in the git repo that you forked at the start of the course (e.g., `FiniteMarkovDecisionProcess`), but if you prefer, you can implement it from scratch or use any code you have written for the course previously (whichever is more convenient for you).\n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Solving for the Optimal Policy\n",
    "\n",
    "Solve for the **Optimal Value Function** and **Optimal Policy** using **Value Iteration**. If you wish, you may use the code in the git repo that you forked at the start of the course (e.g., `rl/dynamic_programming.py`), but if you prefer, you can implement it from scratch or use any code you have written for the course previously (whichever is more convenient for you).\n",
    "\n",
    "---\n",
    "\n",
    "### Part (D): Visualization\n",
    "\n",
    "Plot a graph of the **Optimal Policy** (or print the optimal policy) for the following configuration:  \n",
    "$H = 10$, $W = 30$, $\\alpha = 0.08$, $\\beta = 0.82$, $\\gamma = 0.95$.  \n",
    "\n",
    "Provide an intuitive explanation for this optimal policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in with Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in with Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in with Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Car Sales (Led By: _____)\n",
    "\n",
    "Imagine you own a car which you need to sell within a finite window of $N$ days. At the beginning of each day, you receive an offer from a dealership which is randomly distributed over the interval $[m, M]$, $0 < m < M$, with a known continuous distribution $Q$ on this support; the offers each day are i.i.d. After receiving an offer, you have to decide immediately whether to accept or reject it. If you reject the offer, it is lost, and you have to pay a parking cost for the car of $c \\geq 0$, which you must pay at the end of each day you do not sell the car. After $N$ days, the car has to be sold. The parameters $m$, $M$, and $c$ are all fixed positive real numbers. Your objective is to maximize the sale proceeds.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): MDP Modeling\n",
    "\n",
    "With proper mathematical notation, model this problem fully as an **MDP** by specifying the following:\n",
    "1. **States**: Define the state space\n",
    "2. **Actions**: Specify the possible actions available to the seller\n",
    "3. **Rewards**: Define the reward structure\n",
    "4. **State-Transition Probabilities**: Specify the transition dynamics\n",
    "5. **Discount Factor**: Indicate the discount factor\n",
    "\n",
    "Additionally, discuss what particular kind of MDP this is.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Analytical Solution\n",
    "\n",
    "Solve this MDP analytically for the **optimal policy**. Provide a detailed explanation of the steps used to derive the policy and any key conditions or assumptions required (*Note*: this is to be done mathematically, not using code).\n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Special Case Analysis\n",
    "\n",
    "Consider the case where $c = 0$ and $Q(x) = U_{[m, M]}(x)$ (the uniform distribution on $[m, M]$). Solve for as closed-form a solution of the optimal policy **as possible**. To make this concrete, the functional form of your optimal policy should be explicitly defined but can depend on coefficients that are recursively defined. **You should not have integrals in your solution.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Constrained Consumption (Led By: _____)\n",
    "\n",
    "Consider the following discrete-time MDP for constrained consumption. At $t = 0$, the agent is given a finite amount $x_0 \\in \\mathbb{R}^+$ of a resource. In each time period, the agent can choose to consume any amount of the resource, with the consumption denoted as $c \\in [0, x]$ where $x$ is the amount of the resource remaining at the start of the time period. This consumption results in a reduction of the resource at the start of the next time period:  \n",
    "$$x' = x - c.$$  \n",
    "\n",
    "Consuming a quantity $c$ of the resource provides a utility of consumption equal to $U(c)$, and we adopt the **CRRA utility function**:  \n",
    "$$\n",
    "U(c) = \\frac{c^{1 - \\gamma}}{1 - \\gamma}, \\quad (\\gamma > 0, \\gamma \\neq 1)\n",
    "$$\n",
    "\n",
    "Our goal is to maximize the aggregate discounted utility of consumption until the resource is completely consumed. We assume a discount factor of $\\beta \\in [0, 1]$ when discounting the utility of consumption over any single time period.\n",
    "\n",
    "We model this as a **discrete-time, continuous-state-space, continuous-action-space, stationary, deterministic MDP**, and so our goal is to solve for the **Optimal Value Function** and associated **Optimal Policy**, which will give us the optimal consumption trajectory of the resource. Since this is a stationary MDP, the **State** is simply the amount $x$ of the resource remaining at the start of a time period. The **Action** is the consumption quantity $c$ in that time period. The **Reward** for a time period is $U(c)$ when the consumption in that time period is $c$. The discount factor over each single time period is $\\beta$.\n",
    "\n",
    "We assume that the **Optimal Policy** is given by:  \n",
    "$$\n",
    "c^* = \\theta^* \\cdot x \\quad \\text{for some } \\theta^* \\in [0, 1].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): Closed-form Expression for $V_\\theta(x)$\n",
    "\n",
    "Our first step is to consider a fixed deterministic policy, given by:  \n",
    "$$c = \\theta \\cdot x \\quad \\text{for some fixed } \\theta \\in [0, 1].$$  \n",
    "Derive a closed-form expression for the **Value Function** $V_\\theta(x)$ for a fixed deterministic policy, given by $c = \\theta \\cdot x$. Specifically, you need to express $V_\\theta(x)$ in terms of $\\beta$, $\\gamma$, $\\theta$, and $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Solving for $\\theta^*$\n",
    "\n",
    "Use this closed-form expression for $V_\\theta(x)$ to solve for the $\\theta^*$ which maximizes $V_\\theta(x)$ (thus fetching us the **Optimal Policy** given by $c^* = \\theta^* \\cdot x$).\n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Expression for $V^*(x)$\n",
    "\n",
    "Use this expression for $\\theta^*$ to obtain an expression for the **Optimal Value Function** $V^*(x)$ in terms of only $\\beta$, $\\gamma$, and $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (D): Validation of the Bellman Equation\n",
    "\n",
    "Validate that the **Optimal Policy** (derived in part B) and **Optimal Value Function** (derived in part C) satisfy the **Bellman Optimality Equation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: TD and MC Algorithms (Led By: _____)\n",
    "\n",
    "In this question, we explore the connection between **Temporal Difference (TD)** and **Monte Carlo (MC)** algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): Implementing TD($\\lambda$) Prediction Algorithm\n",
    "\n",
    "Implement the **TD($\\lambda$) Prediction algorithm** from scratch in Python code. First, implement it for the **Tabular case**. Next, implement it for the **Function Approximation case**.  \n",
    "\n",
    "Provide clear and well-commented code for both implementations, and describe any assumptions or simplifications made.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Proof of MC Error as Sum of Discounted TD Errors\n",
    "\n",
    "Prove that the **MC Error** can be written as the sum of discounted TD errors, i.e.,  \n",
    "$$\n",
    "G_t - V(S_t) = \\sum_{u=t}^{T-1} \\gamma^{u-t} \\cdot \\big( R_{u+1} + \\gamma \\cdot V(S_{u+1}) - V(S_u) \\big)\n",
    "$$\n",
    "\n",
    "Work this out from scratch, rather than relying on general results from class or the textbook.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Code Extension\n",
    "\n",
    "Extend `RandomWalkMRP` (in [rl/chapter10/random_walk_mrp.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter10/random_walk_mrp.py)) to `RandomWalkMRP2D` which is a random walk in 2-D with states $\\{i, j) | 0 \\leq i \\leq B_1, 0 \\leq j \\leq B_2\\}$ with terminal states as $(0, j)$ and $(B_1, j)$ for all $j$, $(i, 0)$ and $(i, B_2)$ for all $i$, and with reward of 0 for all $(0, j)$ and for all $(i, 0)$, reward of 1 for all $(B_1, j)$ and for all $(i, B_2)$, and with discrete probabilities of 4 movements - UP, DOWN, LEFT, RIGHT from any non-terminal state. Analyze the convergence of MC and TD on this `RandomWalkMRP2D` much like how we analyzed it for `RandomWalkMRP`, along with plots of similar graphs.\n",
    "\n",
    "Only modify the code where the message `fill in` is noted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in with Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalkMRP2D(FiniteMarkovRewardProcess[int]):\n",
    "    barrier: Tuple[int]\n",
    "    p: Tuple[float]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        barrier: Tuple[int],\n",
    "        p: Tuple[float]\n",
    "    ):\n",
    "        self.barrier = barrier\n",
    "        self.p = p\n",
    "        super().__init__(self.get_transition_map())\n",
    "\n",
    "    def get_transition_map(self) -> Mapping[Tuple[int], Categorical[Tuple[Tuple[int], float]]]:\n",
    "        '''\n",
    "        fill in the code below to define the dictionary, d, and then run the code in the next cell for the visualization\n",
    "        '''\n",
    "        d: Dict[Tuple[int], Categorical[Tuple[Tuple[int], float]]] = _\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_barrier: Tuple[int] = (10, 10)\n",
    "this_p: Tuple[float] = (0.2, 0.3, 0.25, 0.25)\n",
    "random_walk: RandomWalkMRP2D = RandomWalkMRP2D(\n",
    "    barrier=this_barrier,\n",
    "    p=this_p\n",
    ")\n",
    "compare_td_and_mc(\n",
    "    fmrp=random_walk,\n",
    "    gamma=1.0,\n",
    "    mc_episode_length_tol=1e-6,\n",
    "    num_episodes=3500,\n",
    "    learning_rates=[(.1, 1e8, .5), (.05, 1e8, .5)],\n",
    "    initial_vf_dict={s: 0.5 for s in random_walk.non_terminal_states},\n",
    "    plot_batch=7,\n",
    "    plot_start=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7: Double Q-Learning (Led By: _____)\n",
    "\n",
    "It is known that **Q-Learning** can suffer from a maximization bias during finite-sample training. In this problem, we consider the following modification to the **Tabular Q-Learning** algorithm called **Double Q-Learning**:\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm 1: Double Q-Learning**\n",
    "\n",
    "**Initialize** $Q_1(s, a)$ and $Q_2(s, a)$ $\\forall s \\in \\mathcal{N}$, $a \\in \\mathcal{A}$  \n",
    "**yield** estimate of $Q$\n",
    "\n",
    "**while** True **do**  \n",
    "&emsp; **select** initial state $s_0$, **set** $t = 0$  \n",
    "&emsp; **while** $s_t \\in \\mathcal{N}$ **do**  \n",
    "&emsp;&emsp; **select** $a_t$ using $\\epsilon$-greedy based on this greedy policy: $\\pi(s) = \\arg \\max_a \\big( Q_1(s_t, a) + Q_2(s_t, a) \\big)$  \n",
    "&emsp;&emsp; **observe** $(r_t, s_{t+1})$  \n",
    "&emsp;&emsp; **if** with 0.5 probability **then**  \n",
    "&emsp;&emsp;&emsp; $Q_1(s_t, a_t) \\leftarrow Q_1(s_t, a_t) + \\alpha \\big( r_t + \\gamma Q_2(s_{t+1}, \\arg \\max_a Q_2(s_{t+1}, a)) - Q_1(s_t, a_t) \\big)$  \n",
    "\n",
    "&emsp;&emsp; **else**  \n",
    "&emsp;&emsp;&emsp; $Q_2(s_t, a_t) \\leftarrow Q_2(s_t, a_t) + \\alpha \\big( r_t + \\gamma Q_1(s_{t+1}, \\arg \\max_a Q_1(s_{t+1}, a)) - Q_2(s_t, a_t) \\big)$  \n",
    "\n",
    "&emsp;&emsp; $t = t + 1$  \n",
    "&emsp;&emsp; $s_t = s_{t+1}$  \n",
    "\n",
    "**yield** estimate of $Q$\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm 2: Q-Learning**\n",
    "\n",
    "**Initialize** $Q(s, a)$ $\\forall s \\in \\mathcal{N}$, $a \\in \\mathcal{A}$  \n",
    "**yield** $Q$\n",
    "\n",
    "**while** True **do**  \n",
    "&emsp; **select** initial state $s_0$, **set** $t = 0$  \n",
    "&emsp; **while** $s_t \\in \\mathcal{N}$ **do**  \n",
    "&emsp;&emsp; **select** $a_t$ using $\\epsilon$-greedy based on this greedy policy: $\\pi(s) = \\arg \\max_a Q(s_t, a)$  \n",
    "&emsp;&emsp; **observe** $(r_t, s_{t+1})$  \n",
    "&emsp;&emsp; $Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\big( r_t + \\gamma Q(s_{t+1}, \\arg \\max_a Q(s_{t+1}, a)) - Q(s_t, a_t) \\big)$  \n",
    "&emsp;&emsp; $t = t + 1$  \n",
    "&emsp;&emsp; $s_t = s_{t+1}$  \n",
    "\n",
    "**yield** $Q$\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions:\n",
    "The code skeleton for this problem is provided below. Implement the following functions: `double_q_learning` and `q_learning`. Once implemented, you can run the code. You will get a graph of the estimated q-value plotted against the episode number. Comment on your observations, and explain the benefits/drawbacks of the double q-learning algorithm for general MDPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from typing import Mapping, Iterator, TypeVar, Tuple, Dict, Iterable, Generic\n",
    "\n",
    "# RL imports (adapt or remove if you don't have the same environment):\n",
    "from rl.distribution import (\n",
    "    Distribution, Constant, Gaussian, Choose, SampledDistribution, Categorical\n",
    ")\n",
    "from rl.markov_process import NonTerminal, State, Terminal\n",
    "from rl.markov_decision_process import MarkovDecisionProcess\n",
    "from rl.td import epsilon_greedy_action\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Tabular Q-value function approximation (done for you)\n",
    "# -----------------------------------------------------------------------\n",
    "class TabularQValueFunctionApprox(Generic[S, A]):\n",
    "    \"\"\"\n",
    "    A basic implementation of a tabular function approximation \n",
    "    with constant learning rate of 0.1\n",
    "    Also tracks the number of updates per (state, action).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.counts: Mapping[Tuple[NonTerminal[S], A], int] = defaultdict(int)\n",
    "        self.values: Mapping[Tuple[NonTerminal[S], A], float] = defaultdict(float)\n",
    "    \n",
    "    def update(self, k: Tuple[NonTerminal[S], A], target: float) -> None:\n",
    "        alpha = 0.1\n",
    "        old_val = self.values[k]\n",
    "        self.values[k] = (1 - alpha) * old_val + alpha * target\n",
    "        self.counts[k] += 1\n",
    "    \n",
    "    def __call__(self, x: Tuple[NonTerminal[S], A]) -> float:\n",
    "        return self.values[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Double Q-Learning (fill in)\n",
    "# -----------------------------------------------------------------------\n",
    "def double_q_learning(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: Distribution[NonTerminal[S]],\n",
    "    gamma: float\n",
    ") -> Iterator[TabularQValueFunctionApprox[S, A]]:\n",
    "    \"\"\"\n",
    "    Implements Double Q-Learning as described:\n",
    "      1) We keep two Q-tables, Q1 and Q2.\n",
    "      2) We choose actions epsilon-greedily with respect to Q1+Q2.\n",
    "      3) With 50% chance we update Q1 using next-action chosen by max of Q2,\n",
    "         otherwise update Q2 using next-action chosen by max of Q1.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Standard Q-Learning (fill in)\n",
    "# -----------------------------------------------------------------------\n",
    "def q_learning(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: Distribution[NonTerminal[S]],\n",
    "    gamma: float\n",
    ") -> Iterator[TabularQValueFunctionApprox[S, A]]:\n",
    "    \"\"\"\n",
    "    Standard Q-Learning:\n",
    "      1) Keep one Q table\n",
    "      2) Epsilon-greedy wrt that table\n",
    "      3) Update Q((s,a)) with  r + gamma * max_{a'} Q((s_next, a'))\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# The MDP: States A,B and actions a1,a2,b1,...,bn (don't modify anything anymore, just run to get the graphs)\n",
    "# -----------------------------------------------------------------------\n",
    "@dataclass(frozen=True)\n",
    "class P1State:\n",
    "    \"\"\"\n",
    "    The MDP state, storing whether we are in \"A\" or \"B\".\n",
    "    \"\"\"\n",
    "    name: str\n",
    "\n",
    "class P1MDP(MarkovDecisionProcess[P1State, str]):\n",
    "    \n",
    "    def __init__(self, n: int):\n",
    "        self.n = n\n",
    "\n",
    "    def actions(self, state: NonTerminal[P1State]) -> Iterable[str]:\n",
    "        \"\"\"\n",
    "        Return the actions available from this state.\n",
    "          - if state is A => [\"a1\", \"a2\"]\n",
    "          - if state is B => [\"b1\", ..., \"bn\"]\n",
    "        \"\"\"\n",
    "        if state.state.name == \"A\":\n",
    "            return [\"a1\", \"a2\"]\n",
    "        else:\n",
    "            return [f\"b{i}\" for i in range(1, self.n+1)]\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        state: NonTerminal[P1State],\n",
    "        action: str\n",
    "    ) -> Distribution[Tuple[State[P1State], float]]:\n",
    "        \"\"\"\n",
    "        Return the distribution of (next state, reward) from (state, action):\n",
    "          - A + a1 => reward 0, next state B\n",
    "          - A + a2 => reward 0, next state terminal\n",
    "          - B + b_i => reward ~ Normal(-0.1,1), next state terminal\n",
    "        \"\"\"\n",
    "        if state.state.name == \"A\":\n",
    "            if action == \"a1\":\n",
    "                return Constant((NonTerminal(P1State(\"B\")), 0.0))\n",
    "            else:\n",
    "                return Constant((Terminal(P1State(\"T\")), 0.0))\n",
    "        else:\n",
    "            # For B + b_i => reward ~ N(-0.1,1), then terminal\n",
    "            def sampler():\n",
    "                r = np.random.normal(loc=-0.1, scale=1.0)\n",
    "                return (Terminal(P1State(\"T\")), r)\n",
    "            return SampledDistribution(sampler)\n",
    "\n",
    "def run_double_q_once(mdp, start_dist, gamma=1.0, episodes=400):\n",
    "    \"\"\"\n",
    "    Runs one 'chain' of Double Q-Learning for 'episodes' episodes,\n",
    "    returning a list of Q-values for Q((A,a1)) at the end of each episode.\n",
    "    \"\"\"\n",
    "    dq_iter = double_q_learning(mdp, start_dist, gamma)  # generator\n",
    "    vals = []\n",
    "    for _ in range(episodes):\n",
    "        Q1 = next(dq_iter)\n",
    "        # record Q1((A,a1)) each time\n",
    "        qA1 = Q1((NonTerminal(P1State(\"A\")), \"a1\"))\n",
    "        vals.append(qA1)\n",
    "    return vals\n",
    "\n",
    "def run_q_once(mdp, start_dist, gamma=1.0, episodes=400):\n",
    "    \"\"\"\n",
    "    Same but for standard Q-Learning\n",
    "    \"\"\"\n",
    "    q_iter = q_learning(mdp, start_dist, gamma)\n",
    "    vals = []\n",
    "    for _ in range(episodes):\n",
    "        Q = next(q_iter)\n",
    "        qA1 = Q((NonTerminal(P1State(\"A\")), \"a1\"))\n",
    "        vals.append(qA1)\n",
    "    return vals\n",
    "\n",
    "def main():\n",
    "    # For reproducibility\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    n = 10\n",
    "    mdp = P1MDP(n)\n",
    "    # Always start in A, as a NonTerminal\n",
    "    start_dist = Constant(NonTerminal(P1State(\"A\")))\n",
    "\n",
    "    N_RUNS = 100\n",
    "    N_EPISODES = 400\n",
    "\n",
    "    all_dbl = []\n",
    "    all_std = []\n",
    "\n",
    "    for _ in range(N_RUNS):\n",
    "        dbl_vals = run_double_q_once(mdp, start_dist, gamma=1.0, episodes=N_EPISODES)\n",
    "        std_vals = run_q_once(mdp, start_dist, gamma=1.0, episodes=N_EPISODES)\n",
    "        all_dbl.append(dbl_vals)\n",
    "        all_std.append(std_vals)\n",
    "\n",
    "    arr_dbl = np.array(all_dbl)\n",
    "    arr_std = np.array(all_std)\n",
    "\n",
    "    avg_dbl = np.mean(arr_dbl, axis=0)\n",
    "    avg_std = np.mean(arr_std, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(avg_dbl, label='Double Q-Learning: Q(A,a1)')\n",
    "    plt.plot(avg_std, label='Q-Learning: Q(A,a1)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Estimated Q-value')\n",
    "    plt.title('Average Q(A,a1) over 100 runs, n=10')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8: Dice Rolling Game (Led By: _____)\n",
    "\n",
    "Consider the following dice game. You start with $N$ $K$-sided dice on the table, and no dice in your hand. The values on the dice faces are $\\{1, 2, ..., K\\}$. While you have dice remaining on the table, the game proceeds as follows:\n",
    "\n",
    "1. **Roll all the dice on the table.**  \n",
    "2. **Select a nonempty subset of the dice on the table to move to your hand.**  \n",
    "   - The dice you move to your hand keep the value which they were just rolled.  \n",
    "   - For example, if your hand is $\\{1, 3\\}$ and you roll $\\{2, 2, 3, 4\\}$ on the table, and you decide to move the dice with $3$ and $4$ to your hand, you will now have $\\{1, 3, 3, 4\\}$ in your hand.\n",
    "\n",
    "The game ends when you have no dice on the table left to roll. Your score for the game is then calculated as the sum of the values of dice in your hand **if you have at least $C$ 1’s in your hand**, and zero otherwise. For example:\n",
    "- For $N = K = 4$ and $C = 2$, the score corresponding to a hand containing $\\{1, 3, 1, 4\\}$ would be $9$, while the score corresponding to a hand containing $\\{4, 1, 3, 4\\}$ would be $0$.\n",
    "\n",
    "Your goal is to **maximize your score** at the end of the game.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): MDP Modeling\n",
    "\n",
    "With proper mathematical notation, model this as a **Finite MDP** specifying the following:\n",
    "- **States**\n",
    "- **Actions**\n",
    "- **Rewards**\n",
    "- **State-Transition Probabilities**\n",
    "- **Discount Factor**\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Python Implementation\n",
    "\n",
    "Implement this MDP in Python. If you wish, you may use the code in the git repo that you forked at the start of the course (e.g., `FiniteMarkovDecisionProcess`), but if you prefer, you can implement it from scratch or use any code you have written for the course previously (whichever is more convenient for you). You should implement this for the **general case**, specifically your MDP implementation should take as parameters $N$, $K$, $C$.\n",
    "\n",
    "For $N = 6$, $K = 4$, $C = 1$, use the `dynamic_programming.py` library (or your own code if you chose not to implement it within the class library) to solve for the **optimal value function**, and present the following values:\n",
    "\n",
    "1. The **expected score** of the game playing optimally, calculated using your code, not analytically.\n",
    "2. The **optimal action** when rolling $\\{1, 2, 2, 3, 3, 4\\}$ on the first roll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in with Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Score:\n",
    "\n",
    "Optimal Action:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EVERYTHING_PYTHON",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
